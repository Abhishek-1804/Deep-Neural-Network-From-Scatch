{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep neural network from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIVZZM4T1AOgU6oiROZTLe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhidp55/Deep-Neural-Network-From-Scatch/blob/main/Deep_neural_network_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9qw_-gKkzDG"
      },
      "source": [
        "#Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15qq4Pqx3fWM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lndll_ayk3Ob"
      },
      "source": [
        "#Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9PsC2su7KGh"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz04Rgpak68l"
      },
      "source": [
        "##One hot encoding target features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GV1bAfqCTyJ"
      },
      "source": [
        "def one_hot_array(Y):\n",
        "    b = np.zeros((Y.size, Y.max() + 1))\n",
        "    b[np.arange(Y.size), Y] = 1\n",
        "    return b.T"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-6hMTYCCal9"
      },
      "source": [
        "y_train,y_test= one_hot_array(y_train), one_hot_array(y_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-DuBmWwlBF2"
      },
      "source": [
        "## Resizing and standardizing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcJ10ssvDW53",
        "outputId": "0246b57e-d6db-49cc-96af-910d7b88828b"
      },
      "source": [
        "train_x_flatten = x_train.reshape(x_train.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = x_test.reshape(x_test.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "x_train = train_x_flatten/255.\n",
        "x_test = test_x_flatten/255.\n",
        "\n",
        "print (\"train_x's shape: \" + str(x_train.shape))\n",
        "print (\"test_x's shape: \" + str(x_test.shape))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x's shape: (784, 60000)\n",
            "test_x's shape: (784, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf4axNaMG6DR"
      },
      "source": [
        "x_train = x_train[:, 0:5000]\n",
        "x_test = x_test[:, 5000:10000]\n",
        "y_train = y_train[:, 0:5000]\n",
        "y_test = y_test[:, 5000:10000]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhtSvpF4Cmaq",
        "outputId": "8e0e8c0d-bc71-4d53-8917-3d9b7977195a"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 5000)\n",
            "(784, 5000)\n",
            "(10, 5000)\n",
            "(10, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf0XdnpilJBa"
      },
      "source": [
        "#Initializing weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNme5EIEEe2U"
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01 \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        \n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJHeplLnlQ-Y"
      },
      "source": [
        "#Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMZHOX583Vh2"
      },
      "source": [
        "def softmax(Z):\n",
        "    t = np.exp(Z)\n",
        "    t = t / t.sum(axis=0, keepdims=True)\n",
        "    return t\n",
        "\n",
        "def sigmoid(Z):\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    return A\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)    \n",
        "    assert(A.shape == Z.shape)\n",
        "    return A"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiUiFC8tlVqY"
      },
      "source": [
        "#Forward propogation\n",
        "-linear forward activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwFyN2_sJchh"
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = relu(Z)\n",
        "        \n",
        "    elif activation == \"softmax\":\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = softmax(Z)\n",
        "    \n",
        "    # Some assertions to check that shapes are right\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    \n",
        "    # Cache the necessary values for back propagation later\n",
        "    cache = (A_prev, W, b, Z)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWggx3mllaQU"
      },
      "source": [
        "##Forward prop and saving caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ETJMeDlK96V"
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of hidden layers in the neural network\n",
        "    \n",
        "    # Hidden layers 1 to L-1 will be Relu.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation=\"relu\")\n",
        "        caches.append(cache)\n",
        "        \n",
        "    # Output layer L will be softmax\n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation=\"softmax\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (10, X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRWmgbKElj6-"
      },
      "source": [
        "##Computing cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-Cmt7dwK2it"
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    \n",
        "    cost = -1/m * np.sum(np.multiply(Y, np.log(AL)))    \n",
        "    cost = np.squeeze(cost)      # To coerce data from [[17]] into 17\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekP9guSZlmrJ"
      },
      "source": [
        "#Backpropogation\n",
        "-linear backwards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKfBfLYILjiA"
      },
      "source": [
        "def linear_backward(dZ, A_prev, W, b):\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
        "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLXlrUhlrul"
      },
      "source": [
        "##Activation functions(derivative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuj_0GMZLjke"
      },
      "source": [
        "def relu_backward(dA, cache):\n",
        "    A_prev, W, b, Z = cache\n",
        "    \n",
        "    # Compute dZ\n",
        "    dZ = np.array(dA, copy=True) # convert dz to a numpy array\n",
        "    dZ[Z <= 0] = 0\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    # Compute dA_prev, dW, db\n",
        "    dA_prev, dW, db = linear_backward(dZ, A_prev, W, b)\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def softmax_backward(AL, Y, cache):\n",
        "    A_prev, W, b, Z = cache\n",
        "    \n",
        "    # Compute dZ\n",
        "    dZ = AL - Y\n",
        "    \n",
        "    # Compute dA_prev, dW, db\n",
        "    dA_prev, dW, db = linear_backward(dZ, A_prev, W, b)\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1PA0LlWmpY_"
      },
      "source": [
        "##Backward activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZQlwHBsLjml"
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Backpropagation at layer L-1\n",
        "    # The activation is softmax at layer L-1\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = softmax_backward(AL, Y, current_cache)\n",
        "    \n",
        "    # Backpropagation from layers L-2 to 1\n",
        "    # The activations are relu at all these layers\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = relu_backward(grads[\"dA\" + str(l+1)], current_cache)\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbtS92hyl1EL"
      },
      "source": [
        "##Updating weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFaNLzc_Ljph"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxZ1AdUzl4od"
      },
      "source": [
        "#Implementing N-layered neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCSjuDY0Ljr3"
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    costs = []                         \n",
        "\n",
        "    # Step a: Initialise Parameters\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    # Iterative loops of gradient descent\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Step b: Forward Propagation\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        \n",
        "        # Step c: Compute cost\n",
        "        cost = compute_cost(AL, Y)\n",
        "        \n",
        "        # Step d: Backward Propagation\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        \n",
        "        # Step e: Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 10 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "wUxWCsBeLjwz",
        "outputId": "ca92bb70-4701-4427-fa3b-362497fbce1f"
      },
      "source": [
        "layers_dims = [784, 10, 10]\n",
        "parameters = L_layer_model(x_train, y_train, layers_dims, learning_rate = 0.05, num_iterations = 2500, print_cost=True)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.301968\n",
            "Cost after iteration 100: 2.202133\n",
            "Cost after iteration 200: 1.363735\n",
            "Cost after iteration 300: 0.815412\n",
            "Cost after iteration 400: 0.612682\n",
            "Cost after iteration 500: 0.509682\n",
            "Cost after iteration 600: 0.444023\n",
            "Cost after iteration 700: 0.398038\n",
            "Cost after iteration 800: 0.364673\n",
            "Cost after iteration 900: 0.339668\n",
            "Cost after iteration 1000: 0.320211\n",
            "Cost after iteration 1100: 0.304549\n",
            "Cost after iteration 1200: 0.291563\n",
            "Cost after iteration 1300: 0.280571\n",
            "Cost after iteration 1400: 0.271077\n",
            "Cost after iteration 1500: 0.262719\n",
            "Cost after iteration 1600: 0.255243\n",
            "Cost after iteration 1700: 0.248419\n",
            "Cost after iteration 1800: 0.242130\n",
            "Cost after iteration 1900: 0.236317\n",
            "Cost after iteration 2000: 0.230877\n",
            "Cost after iteration 2100: 0.225757\n",
            "Cost after iteration 2200: 0.220917\n",
            "Cost after iteration 2300: 0.216313\n",
            "Cost after iteration 2400: 0.211928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MnoKXBfl_nN"
      },
      "source": [
        "#Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfpPnpIaLjur"
      },
      "source": [
        "def predict(X, parameters):\n",
        "    # Forward propagation\n",
        "    probabilities, caches = L_model_forward(X, parameters)\n",
        "    \n",
        "    # Calculate Predictions (the highest probability for a given example is coded as 1, otherwise 0)\n",
        "    predictions = (probabilities == np.amax(probabilities, axis=0, keepdims=True))\n",
        "    predictions = predictions.astype(float)\n",
        "\n",
        "    return predictions, probabilities\n",
        "\n",
        "def evaluate_prediction(predictions, Y):\n",
        "    m = Y.shape[1]\n",
        "    predictions_class = predictions.argmax(axis=0).reshape(1, m)\n",
        "    Y_class = Y.argmax(axis=0).reshape(1, m)\n",
        "    \n",
        "    return np.sum((predictions_class == Y_class) / (m))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQTowhOdCv-p",
        "outputId": "fb23f526-b9ab-4756-f419-eef63b0ab248"
      },
      "source": [
        "pred_train, probs_train = predict(x_train, parameters)\n",
        "print(\"Train set accuracy is: \" + str(evaluate_prediction(pred_train, y_train)*100) )\n",
        "pred_test, probs_test = predict(x_test, parameters)\n",
        "print(\"Test set accuracy is: \" + str(evaluate_prediction(pred_test, y_test)*100) )"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set accuracy is: 94.28\n",
            "Test set accuracy is: 92.78000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5COjJQdbsy9n"
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": []
    }
  ]
}