{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing Deep neural nets from scratch using numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIVZZM4T1AOgU6oiROZTLe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhidp55/Deep-Neural-Network-From-Scatch/blob/main/Implementing_Deep_neural_nets_from_scratch_using_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9qw_-gKkzDG"
      },
      "source": [
        "#Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15qq4Pqx3fWM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lndll_ayk3Ob"
      },
      "source": [
        "#Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9PsC2su7KGh"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz04Rgpak68l"
      },
      "source": [
        "##One hot encoding target features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GV1bAfqCTyJ"
      },
      "source": [
        "def one_hot_array(Y):\n",
        "    b = np.zeros((Y.size, Y.max() + 1))\n",
        "    b[np.arange(Y.size), Y] = 1\n",
        "    return b.T"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-6hMTYCCal9"
      },
      "source": [
        "y_train,y_test= one_hot_array(y_train), one_hot_array(y_test)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-DuBmWwlBF2"
      },
      "source": [
        "## Resizing and standardizing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcJ10ssvDW53",
        "outputId": "4b96cfab-e510-407c-d735-261619e1e4bf"
      },
      "source": [
        "train_x_flatten = x_train.reshape(x_train.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = x_test.reshape(x_test.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "x_train = train_x_flatten/255.\n",
        "x_test = test_x_flatten/255.\n",
        "\n",
        "print (\"train_x's shape: \" + str(x_train.shape))\n",
        "print (\"test_x's shape: \" + str(x_test.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x's shape: (784, 60000)\n",
            "test_x's shape: (784, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf4axNaMG6DR"
      },
      "source": [
        "x_train = x_train[:, 0:5000]\n",
        "x_test = x_test[:, 5000:10000]\n",
        "y_train = y_train[:, 0:5000]\n",
        "y_test = y_test[:, 5000:10000]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhtSvpF4Cmaq",
        "outputId": "b9704afc-0459-4e11-c7f8-8a1a32fa98c2"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 5000)\n",
            "(784, 5000)\n",
            "(10, 5000)\n",
            "(10, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf0XdnpilJBa"
      },
      "source": [
        "#Initializing weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNme5EIEEe2U"
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01 \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        \n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJHeplLnlQ-Y"
      },
      "source": [
        "#Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMZHOX583Vh2"
      },
      "source": [
        "def softmax(Z):\n",
        "    t = np.exp(Z)\n",
        "    t = t / t.sum(axis=0, keepdims=True)\n",
        "    return t\n",
        "\n",
        "def sigmoid(Z):\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    return A\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)    \n",
        "    assert(A.shape == Z.shape)\n",
        "    return A"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiUiFC8tlVqY"
      },
      "source": [
        "#Forward propogation\n",
        "-linear forward activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwFyN2_sJchh"
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = relu(Z)\n",
        "        \n",
        "    elif activation == \"softmax\":\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = softmax(Z)\n",
        "    \n",
        "    # Some assertions to check that shapes are right\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    \n",
        "    # Cache the necessary values for back propagation later\n",
        "    cache = (A_prev, W, b, Z)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWggx3mllaQU"
      },
      "source": [
        "##Forward prop and saving caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ETJMeDlK96V"
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of hidden layers in the neural network\n",
        "    \n",
        "    # Hidden layers 1 to L-1 will be Relu.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation=\"relu\")\n",
        "        caches.append(cache)\n",
        "        \n",
        "    # Output layer L will be softmax\n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation=\"softmax\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (10, X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRWmgbKElj6-"
      },
      "source": [
        "##Computing cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-Cmt7dwK2it"
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    \n",
        "    cost = -1/m * np.sum(np.multiply(Y, np.log(AL)))    \n",
        "    cost = np.squeeze(cost)      # To coerce data from [[17]] into 17\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekP9guSZlmrJ"
      },
      "source": [
        "#Backpropogation\n",
        "-linear backwards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKfBfLYILjiA"
      },
      "source": [
        "def linear_backward(dZ, A_prev, W, b):\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
        "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLXlrUhlrul"
      },
      "source": [
        "##Activation functions(derivative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuj_0GMZLjke"
      },
      "source": [
        "def relu_backward(dA, cache):\n",
        "    A_prev, W, b, Z = cache\n",
        "    \n",
        "    # Compute dZ\n",
        "    dZ = np.array(dA, copy=True) # convert dz to a numpy array\n",
        "    dZ[Z <= 0] = 0\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    # Compute dA_prev, dW, db\n",
        "    dA_prev, dW, db = linear_backward(dZ, A_prev, W, b)\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def softmax_backward(AL, Y, cache):\n",
        "    A_prev, W, b, Z = cache\n",
        "    \n",
        "    # Compute dZ\n",
        "    dZ = AL - Y\n",
        "    \n",
        "    # Compute dA_prev, dW, db\n",
        "    dA_prev, dW, db = linear_backward(dZ, A_prev, W, b)\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1PA0LlWmpY_"
      },
      "source": [
        "##Backward activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZQlwHBsLjml"
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Backpropagation at layer L-1\n",
        "    # The activation is softmax at layer L-1\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = softmax_backward(AL, Y, current_cache)\n",
        "    \n",
        "    # Backpropagation from layers L-2 to 1\n",
        "    # The activations are relu at all these layers\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = relu_backward(grads[\"dA\" + str(l+1)], current_cache)\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbtS92hyl1EL"
      },
      "source": [
        "##Updating weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFaNLzc_Ljph"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxZ1AdUzl4od"
      },
      "source": [
        "#Implementing N-layered neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCSjuDY0Ljr3"
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    costs = []                         \n",
        "\n",
        "    # Step a: Initialise Parameters\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    # Iterative loops of gradient descent\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Step b: Forward Propagation\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        \n",
        "        # Step c: Compute cost\n",
        "        cost = compute_cost(AL, Y)\n",
        "        \n",
        "        # Step d: Backward Propagation\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        \n",
        "        # Step e: Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 10 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "wUxWCsBeLjwz",
        "outputId": "c498c733-e1b5-475e-a733-fb91d75f8648"
      },
      "source": [
        "layers_dims = [784, 10, 10]\n",
        "parameters = L_layer_model(x_train, y_train, layers_dims, learning_rate = 0.05, num_iterations = 2500, print_cost=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.302788\n",
            "Cost after iteration 100: 2.284192\n",
            "Cost after iteration 200: 1.782586\n",
            "Cost after iteration 300: 0.949263\n",
            "Cost after iteration 400: 0.646247\n",
            "Cost after iteration 500: 0.515599\n",
            "Cost after iteration 600: 0.442501\n",
            "Cost after iteration 700: 0.394159\n",
            "Cost after iteration 800: 0.359630\n",
            "Cost after iteration 900: 0.333686\n",
            "Cost after iteration 1000: 0.313337\n",
            "Cost after iteration 1100: 0.296612\n",
            "Cost after iteration 1200: 0.282720\n",
            "Cost after iteration 1300: 0.270956\n",
            "Cost after iteration 1400: 0.260741\n",
            "Cost after iteration 1500: 0.251684\n",
            "Cost after iteration 1600: 0.243524\n",
            "Cost after iteration 1700: 0.236095\n",
            "Cost after iteration 1800: 0.229250\n",
            "Cost after iteration 1900: 0.222942\n",
            "Cost after iteration 2000: 0.217126\n",
            "Cost after iteration 2100: 0.211663\n",
            "Cost after iteration 2200: 0.206531\n",
            "Cost after iteration 2300: 0.201697\n",
            "Cost after iteration 2400: 0.197134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhc5X328e9PMxpJo83aLC8SFsYLmDXgsJQlTiAJJhSykISkZG1emjRpkq4hbba2L33TpjQNTRpKswAphSyQYAgpSwphCZtNvBu821i2ZUm2Ze3SjJ73j3Nkj4Uky/IcnVnuz3Wda86c88zM79HYuvWc1ZxziIhI/ioIuwAREQmXgkBEJM8pCERE8pyCQEQkzykIRETynIJARCTPKQgk55nZpWb2ath1iGQqBYEEysy2m9kVYdbgnHvaObcwzBqGmdkSM9s1RZ91uZm9YmY9ZvaEmc0Zp22T36bHf80VKes+amZJM+tKmZZMRR9kaigIJOuZWSTsGgDMkxH/p8ysFrgf+DJQDSwHfjzOS+4BfgfUAH8D/MzM6lLWP+ecK0uZngymcglDRvyjlfxjZgVmdpOZbTGzdjP7iZlVp6z/qZntNbMOM3vKzE5PWXeHmX3XzB42s27gzf7I4y/MbLX/mh+bWbHf/qi/wsdr66//KzPbY2a7zewTZubMbN4Y/XjSzG42s2eBHmCumX3MzDaYWaeZbTWzP/LblgK/Amal/GU961g/i0l6N7DOOfdT51wf8DXgbDM7dZQ+LADOBb7qnOt1zt0HrAHec4I1SJZQEEhY/gR4J/AmYBZwAPhOyvpfAfOB6cDLwN0jXv9B4GagHHjGX/Y+4ErgZOAs4KPjfP6obc3sSuDPgCuAecCSCfTlQ8CNfi07gH3A1UAF8DHgm2Z2rnOuG1gK7E75y3r3BH4Wh5nZSWZ2cJzpg37T04FVw6/zP3uLv3yk04GtzrnOlGWrRrR9g5m1mdlGM/uymUUn8HORLKEvU8LySeAzzrldAGb2NWCnmX3IOZdwzv1guKG/7oCZVTrnOvzFDzjnnvXn+8wM4Fb/Fytm9iBwzjifP1bb9wE/dM6tS/nsPzhGX+4Ybu/7Zcr8b8zsUeBSvEAbzbg/i9SGzrmdwLRj1ANQBrSOWNaBF1ajte0Ype1sf/4p4Ay8kDsdbxNTAvh/E6hDsoBGBBKWOcDPh/+SBTYASaDezCJm9nV/U8khYLv/mtqU1782ynvuTZnvwfsFN5ax2s4a8d6jfc5IR7Uxs6Vm9ryZ7ff7dhVH1z7SmD+LCXz2WLrwRiSpKoDO423rnNvqnNvmnBtyzq0B/g647gRqkwyjIJCwvAYsdc5NS5mKnXPNeJt9rsXbPFMJNPmvsZTXB3XZ3D1AQ8rzxgm85nAtZlYE3Af8M1DvnJsGPMyR2kere7yfxVH8TUNd40zDo5d1wNkprysFTvGXj7QOb99G6mjh7DHaDvfBxlgnWUhBIFOh0MyKU6YocBtws/mHNJpZnZld67cvB/qBdiAO/MMU1voT4GNmdpqZxfGOujkeMaAIb7NMwsyWAm9LWd8C1JhZZcqy8X4WR3HO7Rxx9M7IaXhfys+BM8zsPf6O8K8Aq51zr4zynhuBlcBX/e/nXXj7Te7z61lqZvX+/Kn+z+SB4/y5SAZTEMhUeBjoTZm+BnwLWAY8amadwPPABX77u/C2RzcD6/11U8I59yvgVuAJYHPKZ/dP8PWdwGfxAuUA3uhmWcr6V/AO1dzqbwqaxfg/i8n2oxXvqJ+b/TouAK4fXm9mt5nZbSkvuR5Y7Lf9OnCd/x4AlwOr/SO0HsY7LHUqw1kCZroxjcjYzOw0YC1QNHLHrUiu0IhAZAQze5eZFZlZFfCPwIMKAcllCgKR1/sjvHMBtuAdvfOpcMsRCZY2DYmI5DmNCERE8lzWnVlcW1vrmpqawi5DRCSrrFixos05VzfauqwLgqamJpYvXx52GSIiWcXMdoy1TpuGRETynIJARCTPKQhERPKcgkBEJM8pCERE8pyCQEQkzykIRETyXNadRzBZG1s6eWj1HkpjEeJFUe8xFiUei1BWHGVGRTHTy4uIRpSNIpJf8ioIbv31pnHbRAqMOdVxzjlpGlecVs8Vp9UTiyoYRCS3Zd1F5xYvXuwme2bx0JCjL5Gkuz9Jz0Di8GNnX4K9h/poPtDLxpZOXtq+nwM9g8ysLOZL71jEO86ameZeiIhMLTNb4ZxbPNq6vBkRABQUmL85KIp3N8HRJYccv9m4j28+tolP//fLLN/RxFeuXoSZbtMqIrknr4JgoiIFxltOrefS+XXc/MsN/PDZ7cRjEf7y7aeGXZqISNopCMZRGCngq7+/iN6BJN95YguXza/jgrk1YZclIpJW2hN6DGbGV69ZxEnVcb5w32oGEkNhlyQiklYKggmIx6J87ZpFbG/vYdmq3WGXIyKSVgqCCXrzwumcOqOc25/aQrYdaSUiMh4FwQSZGTdeNpeNLV08s7kt7HJERNJGQXAcrjpzJmVFUR7U5iERySEKguNQXBjhrYvqeWRdi3Yai0jOUBAcp6vPmklH7yDPbtHmIRHJDQqC43TJ/FrKiqI8vr4l7FJERNJCQXCciqIRzj+5mue2toddiohIWigIJuGiuTVsbe2m5VBf2KWIiJwwBcEkXHSKd5mJ57ZoVCAi2U9BMAmLZlZQWVLIb7XDWERygIJgEgoKjDc2VfHyzoNhlyIicsIUBJN0xuxKtrR20d2fCLsUEZEToiCYpLMaKnEO1u0+FHYpIiInREEwSWfMrgRgTXNHyJWIiJwYBcEkTS8vZkZFMWt2aT+BiGQ3BcEJOGN2Jas1IhCRLKcgOAGnz6pgW1s3fYPJsEsREZk0BcEJmF9fhnOwtbU77FJERCYtsCAws0Yze8LM1pvZOjP73ChtzMxuNbPNZrbazM4Nqp4gzJ9eDsCmfZ0hVyIiMnnRAN87Afy5c+5lMysHVpjZY8659SltlgLz/ekC4Lv+Y1Zoqo0TKTA27+sKuxQRkUkLbETgnNvjnHvZn+8ENgCzRzS7FrjLeZ4HppnZzKBqSreiaIQ5NXE2tSgIRCR7Tck+AjNrAt4AvDBi1WzgtZTnu3h9WGS0+dPL2NyqIBCR7BV4EJhZGXAf8Hnn3KROwzWzG81suZktb21tTW+BJ2je9DK2t3Xr1pUikrUCDQIzK8QLgbudc/eP0qQZaEx53uAvO4pz7nbn3GLn3OK6urpgip2k+dPLSQw5drTryCERyU5BHjVkwPeBDc65fxmj2TLgw/7RQxcCHc65PUHVFISm2lIAdrT3hFyJiMjkBHnU0MXAh4A1ZrbSX/bXwEkAzrnbgIeBq4DNQA/wsQDrCURTTRyA7RoRiEiWCiwInHPPAHaMNg74dFA1TIVp8RgVxVGNCEQka+nM4jRoqi3ViEBEspaCIA3m1JRqRCAiWUtBkAZzquM0H+xlMKlDSEUk+ygI0mBOTZzkkKP5QG/YpYiIHDcFQRoMH0Kq/QQiko0UBGkwp9o7hHTnfu0nEJHsoyBIg9qyImLRAnZp05CIZCEFQRoUFBgNVSW8phGBiGQhBUGaNFTFNSIQkaykIEiTxqoSXjugEYGIZB8FQZo0VMU52DNIZ99g2KWIiBwXBUGaNFaXANB8UJuHRCS7KAjSpKHKO4T0tf0KAhHJLgqCNGmo8kYEu7SfQESyjIIgTWpKY5QURjQiEJGsoyBIEzPvXAKNCEQk2ygI0qixWucSiEj2URCkUYPOJRCRLKQgSKPGqjidfQk6enUugYhkDwVBGg0fOaRrDolINlEQpNHwuQTaTyAi2URBkEbDZxfryCERySYKgjSqLCmkrCiqEYGIZBUFQRrpXAIRyUYKgjRrqIrr7GIRySoKgjQbHhE458IuRURkQhQEadZYHad7IMnBHp1LICLZQUGQZofPJdB+AhHJEgqCNGvUuQQikmUUBGnWUK2zi0UkuygI0qyiuJDKkkKNCEQkaygIAqBzCUQkmygIAuBdjlojAhHJDgqCADRWxXUugYhkDQVBABqqSugbHKKtayDsUkREjklBEIDG6uFDSLWfQEQyn4IgALovgYhkEwVBAHR2sYhkk8CCwMx+YGb7zGztGOuXmFmHma30p68EVctUKy2KUl0a04hARLJCNMD3vgP4NnDXOG2eds5dHWANoWmoKtHZxSKSFQIbETjnngL2B/X+ma6xKk6zRgQikgXC3kdwkZmtMrNfmdnpYzUysxvNbLmZLW9tbZ3K+iatoaqEXQd7GRrSuQQiktnCDIKXgTnOubOBfwN+MVZD59ztzrnFzrnFdXV1U1bgiWiojjOQGKK1qz/sUkRExhVaEDjnDjnnuvz5h4FCM6sNq550Gz5ySOcSiEimCy0IzGyGmZk/f75fS3tY9aRb4/AhpLp/sYhkuMCOGjKze4AlQK2Z7QK+ChQCOOduA64DPmVmCaAXuN7l0MV5jpxUphGBiGS2wILAOfeBY6z/Nt7hpTmpuDBCbVmRziUQkYwX9lFDOa2xukRnF4tIxlMQBKihKq4RgYhkPAVBgBqrSth9sJekziUQkQymIAhQQ1WcwaSj5VBf2KWIiIxJQRCgw1ch1TWHRCSDKQgCdOQGNdpPICKZS0EQoFnTijGDnRoRiEgGUxAEqCgaYVZlCdvbu8MuRURkTAqCgM2tK2Vrq4JARDKXgiBgp9SVsbW1ixy6eoaI5JgJBYGZvXciy+T15taV0j2QZF+nLkctIplpoiOCL05wmYwwt7YMgC2tXSFXIiIyunEvOmdmS4GrgNlmdmvKqgogEWRhuWJuXSkA29q6+b1TcuZ2CyKSQ4519dHdwHLgGmBFyvJO4E+DKiqXzKgopqQwoh3GIpKxxg0C59wqYJWZ/bdzbhDAzKqARufcgakoMNsVFBhNtaVs1aYhEclQE91H8JiZVZhZNd69hv/TzL4ZYF05ZW5dKVvbNCIQkcw00SCodM4dAt4N3OWcuwC4PLiycssptaW8tr+H/kQy7FJERF5nokEQNbOZwPuAhwKsJyfNrStjyOnicyKSmSYaBH8HPAJscc69ZGZzgU3BlZVbho8c2qIdxiKSgSZ0z2Ln3E+Bn6Y83wq8J6iics3JtV4Q6MghEclEEz2zuMHMfm5m+/zpPjNrCLq4XFFeXMj08iIdOSQiGWmim4Z+CCwDZvnTg/4ymaCTa3XkkIhkpokGQZ1z7ofOuYQ/3QHUBVhXzpmri8+JSIaaaBC0m9kNZhbxpxuA9iALyzUL6ss40DNIa5cuPicimWWiQfBxvENH9wJ7gOuAjwZUU046dUYFAK/s6Qy5EhGRox3P4aMfcc7VOeem4wXD3wZXVu45dUY5AK/uVRCISGaZaBCclXptIefcfuANwZSUm6pKY0wvL2LD3kNhlyIicpSJBkGBf7E5APxrDk3oHAQ5YuGMco0IRCTjTPSX+S3Ac2Y2fFLZe4Gbgykpd502s4I7frudRHKIaER3CRWRzDCh30bOubvwLjjX4k/vds79KMjCctHC+nIGEkNs0/kEIpJBJrx5xzm3HlgfYC057/TZ3pFDa3d3ML++PORqREQ82j4xhebVlVFcWMCaXdphLCKZQ0EwhaKRAk6bWcHa5o6wSxEROUxBMMXOnF3Jut0dDA3pUhMikhkUBFPsjNmVdA8kdQE6EckYCoIpdubsSgDWNB8MuRIREY+CYIrNn15GPBZh5U4FgYhkBgXBFItGCji7YRordh44dmMRkSkQWBCY2Q/8u5mtHWO9mdmtZrbZzFab2blB1ZJpzptTxYY9nfQMJMIuRUQk0BHBHcCV46xfCsz3pxuB7wZYS0Y5d840kkOOVa/pMFIRCV9gQeCcewrYP06Ta4G7nOd5YJqZzQyqnkzyhkbv+n0va/OQiGSAMPcRzAZeS3m+y1/2OmZ2o5ktN7Plra2tU1JckKpKY5xSV8pL28fLSRGRqZEVO4udc7c75xY75xbX1eXGrZIvnFvD8u0HSCSHwi5FRPJcmEHQDDSmPG/wl+WFi06poas/wdrduu6QiIQrzCBYBnzYP3roQqDDObcnxHqm1AUn1wDw3Jb2kCsRkXwX5OGj9wDPAQvNbJeZ/aGZfdLMPuk3eRjYCmwG/hP446BqyUR15UXMn17Gc1sVBCISrsBuN+mc+8Ax1jvg00F9fja4eF4t9760k77BJMWFkbDLEZE8lRU7i3PVmxbU0Tc4xIvbdPSQiIRHQRCiC+fWEIsW8JuN2X9IrIhkLwVBiEpiES44uZonX90XdikikscUBCF788LpbGntZke77k8gIuFQEITsrYvqAXhk3d6QKxGRfKUgCFljdZxFMyt4ZF1L2KWISJ5SEGSAt51ez8s7D7Cvsy/sUkQkDykIMsDSM2biHPxqjTYPicjUUxBkgIUzyllYX86yVbvDLkVE8pCCIENcc84sVuw4wGv7e8IuRUTyjIIgQ1x7ziwAfvG7vLkAq4hkCAVBhmioinPxvBrufek1kkMu7HJEJI8oCDLIDRfMoflgL0+8ojONRWTqKAgyyBWL6pleXsR/vbAj7FJEJI8oCDJIYaSA688/id9sbNVOYxGZMgqCDPOB8xspMOPuF3aGXYqI5AkFQYaZWVnC5adO58cv7aRnIBF2OSKSBxQEGejGy+ZyoGeQ/3pe+wpEJHgKggy0uKmaS+fXcvtTWzUqEJHAKQgy1Ocun09b1wB3P699BSISLAVBhlrcVM0l82r5j6e2aFQgIoFSEGSwz1/hjQp+8My2sEsRkRymIMhgi5uqufL0GXz7ic00H+wNuxwRyVEKggz3patPA+DmX64PuRIRyVUKggzXUBXn00vm8fCavTyzqS3sckQkBykIssD/uWwuc2rifPmBtdpxLCJppyDIAsWFEb7+7rPY3t7N3z+0IexyRCTHKAiyxEWn1HDjZXO558WdPLpO9zYWkfRREGSRP3/rQhbNrOCm+9fQcqgv7HJEJEcoCLJILFrArR84h77BJDfetZy+wWTYJYlIDlAQZJl508v55vvPYdWuDr5w32qc020tReTEKAiy0NtPn8Ffvn0hD6zczbf/d3PY5YhIlouGXYBMzh8vOYXN+7q45bGNVJQU8pHfawq7JBHJUgqCLGVm/NN1Z9HVn+Cry9ZRUhjhfW9sDLssEclC2jSUxQojBXz7g2/gsgV1fOH+1fxIN7IRkUlQEGS5omiE/7jhPN6ycDpf/sVa/vXxjdqBLCLHRUGQA0piEW770Hlcd14D//r4Jr54/xr6Ezq0VEQmRvsIckRhpIBvXHcWMyuL+bf/3czGlk6+e8N51FcUh12aiGS4QEcEZnalmb1qZpvN7KZR1n/UzFrNbKU/fSLIenKdmfHnb1vIv//Bubyyt5Or/+0Znt7UGnZZIpLhAgsCM4sA3wGWAouAD5jZolGa/tg5d44/fS+oevLJVWfO5Od/fDHTSgr50Pdf5P8+tF6bikRkTEGOCM4HNjvntjrnBoB7gWsD/DxJsXBGOcs+cwkfunAO33tmG0u/9TTPb20PuywRyUBBBsFs4LWU57v8ZSO9x8xWm9nPzGzUA+HN7EYzW25my1tbtaljokpiEf7+nWdw58fPZzA5xPW3P89f/HQV+7sHwi5NRDJI2EcNPQg0OefOAh4D7hytkXPudufcYufc4rq6uiktMBe8aUEdj37+TXxqySn84nfNXH7Lk/zw2W0MJIbCLk1EMkCQQdAMpP6F3+AvO8w51+6c6/effg84L8B68lpJLMIXrjyVX372Uk6bWcHfPriey//lSR5Y2czQkM47EMlnQQbBS8B8MzvZzGLA9cCy1AZmNjPl6TWAbr8VsIUzyrn7Exdw58fPp6yokM/du5Krbn2aZat2k1QgiOSlwILAOZcAPgM8gvcL/ifOuXVm9ndmdo3f7LNmts7MVgGfBT4aVD1yhJnxpgV1/PJPLuFf338Og8khPnvP73jLLU/y3y/s1H0ORPKMZdvlCBYvXuyWL18edhk5ZWjI8ej6Fr775GZW7eqgKl7I+xY38sELTmJOTWnY5YlIGpjZCufc4lHXKQhkmHOO57a286PndvDo+haSQ443Lajjhgvn8OaFdUQjYR9bICKTpSCQ47a3o497X9rJPS/upOVQP7VlMa4+axbXnjOLcxqnYWZhlygix0FBIJM2mBziiVf28YuVzTy+YR8DiSHm1MS59uxZXHnGTE6bWa5QEMkCCgJJi0N9g/zP2r08sLKZ325pxzmYPa2Ety6q522L6nnjydUUavORSEZSEEjatXb28+sNLTy2voWnN7cxkBiiojjKxfNquXheLZfOr+Wk6rhGCyIZQkEggeoZSPDUxjb+95UWntnUxu6OPgAaqkq4dL4XDOc3VTNdl8QWCY2CQKaMc45tbd08u7mNpze18dzWdjr7EgA0VpeweE41582pYnFTFQuml1NQoBGDyFRQEEhoEskh1u4+xPLt+1mx4wAvbT9AW5d3VZHy4ihnzq7kzNmVnOFPc6rjCgeRACgIJGM459i5v4cVOw6wYscB1jR38MqeTgaS3gXwyouinD67gjNnV7JoVgXzp5czb3oZxYWRkCsXyW7jBYFuVSlTysyYU1PKnJpS3n1uAwADiSE2tnSytrmDNc0drG3u4M7ndhy+OmqBwUnVcRbUl7Ogvpz59WUsqC9nbl0pRVEFhMiJUhBI6GLRgsObhq73lw0mh9je1s3Gli42tnSyaV8nr+7t5Nev7Dt8cbxIgTF7WglNtaU01cRpqinl5NpS5tTEaayO61BWkQlSEEhGKowUML++nPn15byDIxep7U8k2eYHxOaWTra197C9rZvf7ThAZ3/icLtIgdFQVUJTjRcMDVUlzJ7mP1aVUFMa06GtIj4FgWSVomiEU2dUcOqMiqOWO+do7x5gR3s329q8cNjW3s2O9m5eHhESAMWFBcyeVsLsqjizp5XQUOVNs6aVMKOimLryIu2XkLyhIJCcYGbUlhVRW1bEeXOqX7e+o3eQ5gO97DrQQ/PBXn++l+aDvaxt7hj19p1V8ULqK4qZXlHMjIqilPli6v3ntWVFRHSUk2Q5BYHkhcqSQipLClk0q2LU9T0DCZr9YNh3qJ+WQ33sPdRHy6F+9nX28cqeQ7R19TPy3j0FBtWlMWpKi6gpi1FTVkRNaYy6cu+xpsxbXuuvj8ci2iQlGUdBIALEY9HD+yTGkkgO0d494IVERx8tnf3sO9RHW9cA7V39tHcPsGbXQdq6BugasSlqWHFhATWlRdSWxagujVEVjzEtHmNavJCqeGHKvPc4LR6jVOEhAVMQiExQNFJAfUUx9RXFnNUwftu+wSTt3X5AdA3Q5gfF4efdA+zr7GdjSxcHewboHhj7rnCFEfMCoiQ1ILz5ynghFcWFVJQUUlEcPfLoLyuKFihE5JgUBCIBKC6MeDujp5VMqP1AYoiDvQN09AxyoGeQAz3D8wMc6Bmko3eAA93e8537e1i1y1s+fK7FWGKRAipKopQXpwZFIeWpoTFiWXlxlNJY1Hssiuow3DygIBDJALFoAdPLi5lefnwX5usdSNLZN8ihvkE6ehP+fIJDvd6yQ70JDvUN0pmybPfB3sNt+o8RJABF0QLKirxQKBueioefR163rtRfXzZyWVGUWFShkokUBCJZrCQWoSQWmfSVXfsTyZSQ8B47+xJ09yfo8qeR8519CfZ19tHdljzctndw7E1bqWKRAj9EIpTGopTEIsRjEeKxqP8YoaTQW18SixAv9NcVHVkXj0X89VHihV47bQI7MQoCkTxWFI1QVBahtqzohN4nkRyieyBJ93BYDAdI38gwSR6e7xlI0DOQpGcgyf7u3sPPeweS9AwkXneE1ngiBXY4FOIxLyRKY5GjgqYkFqGkMEJxYYH/6E3D8yWxAoqjEYpjKcv8qaiwIKfDRkEgIicsGimgsqSAypLCtLyfc47+xJAfFAl6B5J0j5jvTQmSo0PkyPPOvgQth/roGUjSN5ikb3DouENmmBkURyOHA6UoJVCOBEvKstjRy1LbFBVGKI5673H4sTBCcXR4XQHRKdw3oyAQkYxjZof/Yq8ujaX1vZ1zDCYdfYkkfQNJev2A8B795wNJ+hJJegeGjixLXT/c3m/XM5CgvXuA/pS2w+0mK1pgFEW9gBh+/OAFJ/GJS+em8afhf1ba31FEJIOZGbGoEYsWUFGcnhHMWIZHNr2Hg8ULiP6EFzD9iSH6/cDoTxz92DeiXd9g8oQ34Y1FQSAiEpDUkU0m07FcIiJ5TkEgIpLnFAQiInlOQSAikucUBCIieU5BICKS5xQEIiJ5TkEgIpLnzLlJXHQjRGbWCuyY5MtrgbY0lpMt8rHf6nN+UJ8nbo5zrm60FVkXBCfCzJY75xaHXcdUy8d+q8/5QX1OD20aEhHJcwoCEZE8l29BcHvYBYQkH/utPucH9TkN8mofgYiIvF6+jQhERGQEBYGISJ7LmyAwsyvN7FUz22xmN4VdT1DMbLuZrTGzlWa23F9WbWaPmdkm/7Eq7DpPhJn9wMz2mdnalGWj9tE8t/rf+2ozOze8yidvjD5/zcya/e96pZldlbLui36fXzWzt4dT9Ykxs0Yze8LM1pvZOjP7nL88Z7/rcfoc7HftnMv5CYgAW4C5QAxYBSwKu66A+rodqB2x7J+Am/z5m4B/DLvOE+zjZcC5wNpj9RG4CvgVYMCFwAth15/GPn8N+ItR2i7y/40XASf7//YjYfdhEn2eCZzrz5cDG/2+5ex3PU6fA/2u82VEcD6w2Tm31Tk3ANwLXBtyTVPpWuBOf/5O4J0h1nLCnHNPAftHLB6rj9cCdznP88A0M5s5NZWmzxh9Hsu1wL3OuX7n3DZgM97/gazinNvjnHvZn+8ENgCzyeHvepw+jyUt33W+BMFs4LWU57sY/4ebzRzwqJmtMLMb/WX1zrk9/vxeoD6c0gI1Vh9z/bv/jL8Z5Acpm/xyrs9m1gS8AXiBPPmuR/QZAvyu8yUI8sklzrlzgaXAp83sstSVzhtP5vQxw/nQR993gVOAc4A9wC3hlhMMMysD7gM+75w7lLouV7/rUfoc6HedL0HQDDSmPG/wl+Uc51yz/7gP+DneMLFleIjsP+4Lr8LAjNXHnP3unVSFxR0AAAUsSURBVHMtzrmkc24I+E+ObBLImT6bWSHeL8S7nXP3+4tz+rserc9Bf9f5EgQvAfPN7GQziwHXA8tCrintzKzUzMqH54G3AWvx+voRv9lHgAfCqTBQY/VxGfBh/4iSC4GOlM0KWW3E9u934X3X4PX5ejMrMrOTgfnAi1Nd34kyMwO+D2xwzv1Lyqqc/a7H6nPg33XYe8mncG/8VXh74LcAfxN2PQH1cS7eEQSrgHXD/QRqgF8Dm4DHgeqwaz3Bft6DNzwexNsm+odj9RHvCJLv+N/7GmBx2PWnsc8/8vu02v+FMDOl/d/4fX4VWBp2/ZPs8yV4m31WAyv96apc/q7H6XOg37UuMSEikufyZdOQiIiMQUEgIpLnFAQiInlOQSAikucUBCIieU5BIBnDzH7rPzaZ2QfT/N5/PdpnBcXM3mlmXwnovf/62K2O+z3PNLM70v2+kh10+KhkHDNbgnelxauP4zVR51xinPVdzrmydNQ3wXp+C1zjnGs7wfd5Xb+C6ouZPQ583Dm3M93vLZlNIwLJGGbW5c9+HbjUv+76n5pZxMy+YWYv+Rfd+iO//RIze9rMlgHr/WW/8C+4t274ontm9nWgxH+/u1M/yz8L9Rtmtta8+zi8P+W9nzSzn5nZK2Z2t3/WJ2b2df968avN7J9H6ccCoH84BMzsDjO7zcyWm9lGM7vaXz7hfqW892h9ucHMXvSX/YeZRYb7aGY3m9kqM3vezOr95e/1+7vKzJ5KefsH8c66l3wT9pl0mjQNT0CX/7gEeChl+Y3Al/z5ImA53rXXlwDdwMkpbYfPMi3BOw2/JvW9R/ms9wCP4d2zoh7YiXdN+CVAB961WwqA5/DO+qzBO4NzeDQ9bZR+fAy4JeX5HcD/+O8zH+/M4OLj6ddotfvzp+H9Ai/0n/878GF/3gG/78//U8pnrQFmj6wfuBh4MOx/B5qmfopONDBEQvQ24Cwzu85/Xon3C3UAeNF512Ef9lkze5c/3+i3ax/nvS8B7nHOJfEuZvYb4I3AIf+9dwGY2UqgCXge6AO+b2YPAQ+N8p4zgdYRy37ivAuGbTKzrcCpx9mvsVwOnAe85A9YSjhyEbaBlPpWAG/1558F7jCznwD3H3kr9gGzJvCZkmMUBJINDPgT59wjRy309iV0j3h+BXCRc67HzJ7E+8t7svpT5pNA1DmXMLPz8X4BXwd8BnjLiNf14v1STzVyZ5xjgv06BgPudM59cZR1g8654c9N4v9/d8590swuAN4BrDCz85xz7Xg/q94Jfq7kEO0jkEzUiXebvmGPAJ8y7/K8mNkC/+qqI1UCB/wQOBXvdoXDBodfP8LTwPv97fV1eLeEHPPqjeZdJ77SOfcw8KfA2aM02wDMG7HsvWZWYGan4F0c8NXj6NdIqX35NXCdmU3336PazOaM92IzO8U594Jz7it4I5fhyxgv4MhVLSWPaEQgmWg1kDSzVXjb17+Ft1nmZX+HbSuj327zf4BPmtkGvF+0z6esux1YbWYvO+f+IGX5z4GL8K7Y6oC/cs7t9YNkNOXAA2ZWjPfX+J+N0uYp4BYzs5S/yHfiBUwF8EnnXJ+ZfW+C/RrpqL6Y2Zfw7kpXgHd10k8DO8Z5/TfMbL5f/6/9vgO8GfjlBD5fcowOHxUJgJl9C2/H6+P+8fkPOed+FnJZYzKzIuA3eHe4G/MwXMlN2jQkEox/AOJhF3EcTgJuUgjkJ40IRETynEYEIiJ5TkEgIpLnFAQiInlOQSAikucUBCIiee7/A2mUlioazQFXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MnoKXBfl_nN"
      },
      "source": [
        "#Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfpPnpIaLjur"
      },
      "source": [
        "def predict(X, parameters):\n",
        "    # Forward propagation\n",
        "    probabilities, caches = L_model_forward(X, parameters)\n",
        "    \n",
        "    # Calculate Predictions (the highest probability for a given example is coded as 1, otherwise 0)\n",
        "    predictions = (probabilities == np.amax(probabilities, axis=0, keepdims=True))\n",
        "    predictions = predictions.astype(float)\n",
        "\n",
        "    return predictions, probabilities\n",
        "\n",
        "def evaluate_prediction(predictions, Y):\n",
        "    m = Y.shape[1]\n",
        "    predictions_class = predictions.argmax(axis=0).reshape(1, m)\n",
        "    Y_class = Y.argmax(axis=0).reshape(1, m)\n",
        "    \n",
        "    return np.sum((predictions_class == Y_class) / (m))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQTowhOdCv-p",
        "outputId": "26c8bc9e-e63a-42e1-fc9a-2c60d82ca55d"
      },
      "source": [
        "pred_train, probs_train = predict(x_train, parameters)\n",
        "print(\"Train set accuracy is: \" + str(evaluate_prediction(pred_train, y_train)*100) )\n",
        "pred_test, probs_test = predict(x_test, parameters)\n",
        "print(\"Test set accuracy is: \" + str(evaluate_prediction(pred_test, y_test)*100) )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set accuracy is: 94.70000000000002\n",
            "Test set accuracy is: 92.70000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5COjJQdbsy9n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}