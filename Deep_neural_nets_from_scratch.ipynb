{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing Deep neural nets from scratch using numpy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2mugpvZmwkc3oTLXS7RbY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhidp55/Deep-Neural-Network-From-Scatch/blob/main/Deep_neural_nets_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9qw_-gKkzDG"
      },
      "source": [
        "#Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15qq4Pqx3fWM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lndll_ayk3Ob"
      },
      "source": [
        "#Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9PsC2su7KGh"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz04Rgpak68l"
      },
      "source": [
        "##One hot encoding target features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GV1bAfqCTyJ"
      },
      "source": [
        "def one_hot_array(Y):\n",
        "    b = np.zeros((Y.size, Y.max() + 1))\n",
        "    b[np.arange(Y.size), Y] = 1\n",
        "    return b.T"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-6hMTYCCal9"
      },
      "source": [
        "y_train,y_test= one_hot_array(y_train), one_hot_array(y_test)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-DuBmWwlBF2"
      },
      "source": [
        "## Resizing and standardizing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcJ10ssvDW53",
        "outputId": "f7ca68a6-f20b-40ce-8e5b-e7b60abb549b"
      },
      "source": [
        "train_x_flatten = x_train.reshape(x_train.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = x_test.reshape(x_test.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "x_train = train_x_flatten/255.\n",
        "x_test = test_x_flatten/255.\n",
        "\n",
        "print (\"train_x's shape: \" + str(x_train.shape))\n",
        "print (\"test_x's shape: \" + str(x_test.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x's shape: (784, 60000)\n",
            "test_x's shape: (784, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf4axNaMG6DR"
      },
      "source": [
        "x_train = x_train[:, 0:5000]\n",
        "x_test = x_test[:, 5000:10000]\n",
        "y_train = y_train[:, 0:5000]\n",
        "y_test = y_test[:, 5000:10000]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhtSvpF4Cmaq",
        "outputId": "cd8f7a9d-dc0d-4594-bd23-567f61ed727e"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 5000)\n",
            "(784, 5000)\n",
            "(10, 5000)\n",
            "(10, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf0XdnpilJBa"
      },
      "source": [
        "#Initializing weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNme5EIEEe2U"
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01 \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        \n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJHeplLnlQ-Y"
      },
      "source": [
        "#Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMZHOX583Vh2"
      },
      "source": [
        "def softmax(Z):\n",
        "    t = np.exp(Z)\n",
        "    t = t / t.sum(axis=0, keepdims=True)\n",
        "    return t\n",
        "\n",
        "def sigmoid(Z):\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    return A\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)    \n",
        "    assert(A.shape == Z.shape)\n",
        "    return A"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiUiFC8tlVqY"
      },
      "source": [
        "#Forward propogation\n",
        "-linear forward activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwFyN2_sJchh"
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = relu(Z)\n",
        "        \n",
        "    elif activation == \"softmax\":\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = softmax(Z)\n",
        "    \n",
        "    # Some assertions to check that shapes are right\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    \n",
        "    # Cache the necessary values for back propagation later\n",
        "    cache = (A_prev, W, b, Z)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWggx3mllaQU"
      },
      "source": [
        "##Forward prop and saving caches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ETJMeDlK96V"
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of hidden layers in the neural network\n",
        "    \n",
        "    # Hidden layers 1 to L-1 will be Relu.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation=\"relu\")\n",
        "        caches.append(cache)\n",
        "        \n",
        "    # Output layer L will be softmax\n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation=\"softmax\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (10, X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRWmgbKElj6-"
      },
      "source": [
        "##Computing cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-Cmt7dwK2it"
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    \n",
        "    cost = -1/m * np.sum(np.multiply(Y, np.log(AL)))    \n",
        "    cost = np.squeeze(cost)      # To coerce data from [[17]] into 17\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekP9guSZlmrJ"
      },
      "source": [
        "#Backpropogation\n",
        "-linear backwards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKfBfLYILjiA"
      },
      "source": [
        "def linear_backward(dZ, A_prev, W, b):\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
        "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GLXlrUhlrul"
      },
      "source": [
        "##Activation functions(derivative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuj_0GMZLjke"
      },
      "source": [
        "def relu_backward(dA, cache):\n",
        "    A_prev, W, b, Z = cache\n",
        "    \n",
        "    # Compute dZ\n",
        "    dZ = np.array(dA, copy=True) # convert dz to a numpy array\n",
        "    dZ[Z <= 0] = 0\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    # Compute dA_prev, dW, db\n",
        "    dA_prev, dW, db = linear_backward(dZ, A_prev, W, b)\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def softmax_backward(AL, Y, cache):\n",
        "    A_prev, W, b, Z = cache\n",
        "    \n",
        "    # Compute dZ\n",
        "    dZ = AL - Y\n",
        "    \n",
        "    # Compute dA_prev, dW, db\n",
        "    dA_prev, dW, db = linear_backward(dZ, A_prev, W, b)\n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1PA0LlWmpY_"
      },
      "source": [
        "##Backward activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZQlwHBsLjml"
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Backpropagation at layer L-1\n",
        "    # The activation is softmax at layer L-1\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = softmax_backward(AL, Y, current_cache)\n",
        "    \n",
        "    # Backpropagation from layers L-2 to 1\n",
        "    # The activations are relu at all these layers\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = relu_backward(grads[\"dA\" + str(l+1)], current_cache)\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbtS92hyl1EL"
      },
      "source": [
        "##Updating weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFaNLzc_Ljph"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxZ1AdUzl4od"
      },
      "source": [
        "#Implementing N-layered neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCSjuDY0Ljr3"
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    costs = []                         \n",
        "\n",
        "    # Step a: Initialise Parameters\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    # Iterative loops of gradient descent\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Step b: Forward Propagation\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        \n",
        "        # Step c: Compute cost\n",
        "        cost = compute_cost(AL, Y)\n",
        "        \n",
        "        # Step d: Backward Propagation\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        \n",
        "        # Step e: Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 10 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "wUxWCsBeLjwz",
        "outputId": "e12d406a-b093-4643-9b3d-d19d3fd6e0a6"
      },
      "source": [
        "layers_dims = [784, 10, 10]\n",
        "parameters = L_layer_model(x_train, y_train, layers_dims, learning_rate = 0.1, num_iterations = 2500, print_cost=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 2.302502\n",
            "Cost after iteration 100: 1.814991\n",
            "Cost after iteration 200: 0.731915\n",
            "Cost after iteration 300: 0.476925\n",
            "Cost after iteration 400: 0.391436\n",
            "Cost after iteration 500: 0.343748\n",
            "Cost after iteration 600: 0.311044\n",
            "Cost after iteration 700: 0.286652\n",
            "Cost after iteration 800: 0.267740\n",
            "Cost after iteration 900: 0.252598\n",
            "Cost after iteration 1000: 0.240144\n",
            "Cost after iteration 1100: 0.229475\n",
            "Cost after iteration 1200: 0.220094\n",
            "Cost after iteration 1300: 0.211659\n",
            "Cost after iteration 1400: 0.203942\n",
            "Cost after iteration 1500: 0.196767\n",
            "Cost after iteration 1600: 0.189976\n",
            "Cost after iteration 1700: 0.183458\n",
            "Cost after iteration 1800: 0.177213\n",
            "Cost after iteration 1900: 0.171300\n",
            "Cost after iteration 2000: 0.165672\n",
            "Cost after iteration 2100: 0.160284\n",
            "Cost after iteration 2200: 0.155122\n",
            "Cost after iteration 2300: 0.150197\n",
            "Cost after iteration 2400: 0.145509\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MnoKXBfl_nN"
      },
      "source": [
        "#Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfpPnpIaLjur"
      },
      "source": [
        "def predict(X, parameters):\n",
        "    # Forward propagation\n",
        "    probabilities, caches = L_model_forward(X, parameters)\n",
        "    \n",
        "    # Calculate Predictions (the highest probability for a given example is coded as 1, otherwise 0)\n",
        "    predictions = (probabilities == np.amax(probabilities, axis=0, keepdims=True))\n",
        "    predictions = predictions.astype(float)\n",
        "\n",
        "    return predictions, probabilities\n",
        "\n",
        "def evaluate_prediction(predictions, Y):\n",
        "    m = Y.shape[1]\n",
        "    predictions_class = predictions.argmax(axis=0).reshape(1, m)\n",
        "    Y_class = Y.argmax(axis=0).reshape(1, m)\n",
        "    \n",
        "    return np.sum((predictions_class == Y_class) / (m))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQTowhOdCv-p",
        "outputId": "a5ad87c0-f421-48f7-dbd6-3d01fede0e50"
      },
      "source": [
        "pred_train, probs_train = predict(x_train, parameters)\n",
        "print(\"Train set error is: \" + str(evaluate_prediction(pred_train, y_train)))\n",
        "pred_test, probs_test = predict(x_test, parameters)\n",
        "print(\"Test set error is: \" + str(evaluate_prediction(pred_test, y_test)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set error is: 0.9664000000000001\n",
            "Test set error is: 0.9278000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}